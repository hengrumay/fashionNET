{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks Exploration: \n",
    "   \n",
    ">1. Setup GPU instance | jupyter env.   \n",
    ">2. Try out using multiple GPUs with Keras on MNIST  \n",
    "    i)  Check Keras.utils for multiple GPUs support  \n",
    "    ii) Test with/out multiple GPUS\n",
    "\n",
    "*h-rm_tan 15Dec2017*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# /home/ubuntu/Expt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refs:  \n",
    "KERAS API\n",
    "- https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html \n",
    "- https://github.com/keras-team/keras/tree/master/keras/utils   \n",
    "\n",
    "This uses mxnet\n",
    "- https://github.com/NVIDIA/keras  \n",
    "\n",
    "Some example code\n",
    "- https://github.com/normanheckscher/mnist-multi-gpu/blob/master/mnist_multi_gpu_keras.py  \n",
    "- https://github.com/kuza55/keras-extras/blob/master/examples/mnist_cnn_multi.py\n",
    "\n",
    "Use TensorFlow Backend instead\n",
    "- https://www.pyimagesearch.com/2017/10/30/how-to-multi-gpu-training-with-keras-python-and-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful 1st test \n",
    "- https://keras.io/utils/#multi_gpu_model\n",
    "\n",
    "#### To get multiple_gpu_model to load up\n",
    "- https://stackoverflow.com/questions/46840037/cannot-import-multi-gpu-model-from-keras-utils\n",
    "\n",
    "NEED TO INSTALL the latest version of Keras from github to get the multi_gpu_model module to load up on import\n",
    "\n",
    "    sudo pip install git+git://github.com/fchollet/keras.git --upgrade\n",
    "    sudo pip3 install git+git://github.com/fchollet/keras.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Test using multi-gpus with MNIST --- simple CNN model without much parameter tweaks\n",
    "REF: https://github.com/normanheckscher/mnist-multi-gpu/blob/master/mnist_multi_gpu_keras.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/introduction-python-deep-learning-library-keras/\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"]=\"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Test load multi_gpu_model\n",
    "from keras.utils import multi_gpu_model\n",
    "# Using TensorFlow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from tensorflow.contrib.keras.api.keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "tensorboard = TensorBoard(log_dir='/home/ubuntu/Expt/MNIST_train', histogram_freq=1,\n",
    "                          write_graph=True, write_images=False, embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 193545795524082108\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11330676327\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18142107889108613369\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:17.0\"\n",
      ", name: \"/gpu:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11330676327\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 14679588593817377482\n",
      "physical_device_desc: \"device: 1, name: Tesla K80, pci bus id: 0000:00:18.0\"\n",
      ", name: \"/gpu:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11330676327\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13058470020451344541\n",
      "physical_device_desc: \"device: 2, name: Tesla K80, pci bus id: 0000:00:19.0\"\n",
      ", name: \"/gpu:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11328684032\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 9302012676846680521\n",
      "physical_device_desc: \"device: 3, name: Tesla K80, pci bus id: 0000:00:1a.0\"\n",
      ", name: \"/gpu:4\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11328684032\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 15485257243617349019\n",
      "physical_device_desc: \"device: 4, name: Tesla K80, pci bus id: 0000:00:1b.0\"\n",
      ", name: \"/gpu:5\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11328621773\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 9452784142784542080\n",
      "physical_device_desc: \"device: 5, name: Tesla K80, pci bus id: 0000:00:1c.0\"\n",
      ", name: \"/gpu:6\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11326691738\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 11296346192563285150\n",
      "physical_device_desc: \"device: 6, name: Tesla K80, pci bus id: 0000:00:1d.0\"\n",
      ", name: \"/gpu:7\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11326691738\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 736592548627202864\n",
      "physical_device_desc: \"device: 7, name: Tesla K80, pci bus id: 0000:00:1e.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "## Check re resources \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "nb_classes = 10\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='valid',\n",
    "                        input_shape=input_shape))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(256, (kernel_size[0], kernel_size[1])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(128, (kernel_size[0], kernel_size[1])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the base model (or \"template\" model).\n",
    "# We recommend doing this with under a CPU device scope,\n",
    "# so that the model's weights are hosted on CPU memory.\n",
    "# Otherwise they may end up hosted on a GPU, which would\n",
    "# complicate weight sharing.\n",
    "with tf.device('/cpu:0'):\n",
    "    Omodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 55s 913us/step - loss: 0.0606 - acc: 0.9838 - val_loss: 0.0922 - val_acc: 0.9803\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 52s 869us/step - loss: 0.0699 - acc: 0.9828 - val_loss: 0.0357 - val_acc: 0.9910\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 52s 864us/step - loss: 0.0772 - acc: 0.9817 - val_loss: 0.0942 - val_acc: 0.9814\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 52s 863us/step - loss: 0.0804 - acc: 0.9811 - val_loss: 0.0529 - val_acc: 0.9904\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 52s 861us/step - loss: 0.0814 - acc: 0.9813 - val_loss: 0.0420 - val_acc: 0.9900\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 52s 859us/step - loss: 0.0865 - acc: 0.9805 - val_loss: 0.0623 - val_acc: 0.9839\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 51s 858us/step - loss: 0.0874 - acc: 0.9806 - val_loss: 0.0660 - val_acc: 0.9904\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 51s 857us/step - loss: 0.0880 - acc: 0.9810 - val_loss: 0.0493 - val_acc: 0.9881\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 51s 857us/step - loss: 0.0843 - acc: 0.9822 - val_loss: 0.0593 - val_acc: 0.9913\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 51s 857us/step - loss: 0.0890 - acc: 0.9815 - val_loss: 0.0669 - val_acc: 0.9903\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 51s 857us/step - loss: 0.0911 - acc: 0.9806 - val_loss: 0.0547 - val_acc: 0.9921\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 51s 856us/step - loss: 0.0970 - acc: 0.9805 - val_loss: 0.0528 - val_acc: 0.9907\n",
      "Test score -- loss: 0.0527517839638\n",
      "Test metrics -- accuracy: 0.9907\n",
      "Total Duration (626.454 sec)\n"
     ]
    }
   ],
   "source": [
    "## Test with 1 GPU --------------------------------\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='rmsprop',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    batch_size = 128\n",
    "    nb_epoch = 12\n",
    "    ngpus=1\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size*ngpus, epochs=nb_epoch,\n",
    "                      verbose=1, validation_data=(X_test, Y_test)) #, callbacks=[tensorboard])\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score -- loss:', score[0])\n",
    "    print('Test metrics -- accuracy:', score[1])\n",
    "    duration = time.time() - start_time\n",
    "    print('Total Duration (%.3f sec)' % duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test with MULTIPLE GPUs --------------------------------\n",
    "\n",
    "# Replicates the model on N--->max8 GPUs.\n",
    "# This assumes that your machine has 8 available GPUs.\n",
    "# ngpus = 8\n",
    "\n",
    "\n",
    "def run_model_parallelgpus(ngpus):\n",
    "    print(\"Using %i GPUs\" %ngpus)\n",
    "    \n",
    "    parallel_model = multi_gpu_model(Omodel, gpus=ngpus)\n",
    "    parallel_model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='rmsprop',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    batch_size = 128\n",
    "    nb_epoch = 12\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    parallel_model.fit(X_train, Y_train, batch_size=batch_size*ngpus, epochs=nb_epoch,\n",
    "                      verbose=1, validation_data=(X_test, Y_test)) #, callbacks=[tensorboard])\n",
    "\n",
    "    score = parallel_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score -- loss:', score[0])\n",
    "    print('Test metrics -- accuracy:', score[1])\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print('Total Duration (%.3f sec)' % duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 29s 490us/step - loss: 0.0343 - acc: 0.9907 - val_loss: 0.0301 - val_acc: 0.9914\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 27s 442us/step - loss: 0.0329 - acc: 0.9908 - val_loss: 0.0314 - val_acc: 0.9917\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 26s 436us/step - loss: 0.0317 - acc: 0.9912 - val_loss: 0.0252 - val_acc: 0.9936\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 26s 436us/step - loss: 0.0342 - acc: 0.9908 - val_loss: 0.0401 - val_acc: 0.9882\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 26s 439us/step - loss: 0.0355 - acc: 0.9904 - val_loss: 0.0537 - val_acc: 0.9903\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 27s 445us/step - loss: 0.0359 - acc: 0.9906 - val_loss: 0.0294 - val_acc: 0.9924\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 26s 438us/step - loss: 0.0367 - acc: 0.9910 - val_loss: 0.0507 - val_acc: 0.9917\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 26s 435us/step - loss: 0.0394 - acc: 0.9904 - val_loss: 0.0502 - val_acc: 0.9932\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 26s 435us/step - loss: 0.0426 - acc: 0.9903 - val_loss: 0.0379 - val_acc: 0.9925\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 26s 440us/step - loss: 0.0413 - acc: 0.9906 - val_loss: 0.0922 - val_acc: 0.9805\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 26s 441us/step - loss: 0.0474 - acc: 0.9898 - val_loss: 0.0700 - val_acc: 0.9860\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 26s 435us/step - loss: 0.0449 - acc: 0.9898 - val_loss: 0.0482 - val_acc: 0.9909\n",
      "Test score -- loss: 0.0482255089384\n",
      "Test metrics -- accuracy: 0.9909\n",
      "Total Duration (322.038 sec)\n"
     ]
    }
   ],
   "source": [
    "run_model_parallelgpus(ngpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 17s 285us/step - loss: 0.0418 - acc: 0.9909 - val_loss: 0.0497 - val_acc: 0.9937\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0357 - acc: 0.9922 - val_loss: 0.0505 - val_acc: 0.9927\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0368 - acc: 0.9922 - val_loss: 0.0537 - val_acc: 0.9911\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0337 - acc: 0.9926 - val_loss: 0.0395 - val_acc: 0.9945\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0380 - acc: 0.9918 - val_loss: 0.0739 - val_acc: 0.9904\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0311 - acc: 0.9935 - val_loss: 0.0453 - val_acc: 0.9914\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 15s 242us/step - loss: 0.0306 - acc: 0.9932 - val_loss: 0.0589 - val_acc: 0.9923\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0318 - acc: 0.9930 - val_loss: 0.0348 - val_acc: 0.9940\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0330 - acc: 0.9930 - val_loss: 0.0692 - val_acc: 0.9931\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0315 - acc: 0.9932 - val_loss: 0.0500 - val_acc: 0.9948\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 0.0404 - acc: 0.9921 - val_loss: 0.1623 - val_acc: 0.9843\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 0.0330 - acc: 0.9931 - val_loss: 0.0681 - val_acc: 0.9908\n",
      "Test score -- loss: 0.0681361730697\n",
      "Test metrics -- accuracy: 0.9908\n",
      "Total Duration (179.739 sec)\n"
     ]
    }
   ],
   "source": [
    "run_model_parallelgpus(ngpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0340 - acc: 0.9927 - val_loss: 0.0469 - val_acc: 0.9940\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0272 - acc: 0.9940 - val_loss: 0.0358 - val_acc: 0.9940\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 8s 142us/step - loss: 0.0260 - acc: 0.9948 - val_loss: 0.0554 - val_acc: 0.9940\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0235 - acc: 0.9950 - val_loss: 0.0397 - val_acc: 0.9901\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 8s 142us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0311 - val_acc: 0.9933\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0216 - acc: 0.9951 - val_loss: 0.0586 - val_acc: 0.9942\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0269 - acc: 0.9947 - val_loss: 0.0325 - val_acc: 0.9924\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0214 - acc: 0.9950 - val_loss: 0.0409 - val_acc: 0.9942\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0231 - acc: 0.9950 - val_loss: 0.0446 - val_acc: 0.9925\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0351 - val_acc: 0.9938\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0203 - acc: 0.9957 - val_loss: 0.0378 - val_acc: 0.9950\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0235 - acc: 0.9951 - val_loss: 0.0483 - val_acc: 0.9946\n",
      "Test score -- loss: 0.0483050177622\n",
      "Test metrics -- accuracy: 0.9946\n",
      "Total Duration (111.724 sec)\n"
     ]
    }
   ],
   "source": [
    "run_model_parallelgpus(ngpus=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Test Summary\n",
    "\n",
    "Using 1 GPU \n",
    "Test score -- loss: 0.0527517839638 |\n",
    "Test metrics -- accuracy: 0.9907 |\n",
    "Total Duration (626.454 sec)\n",
    "\n",
    "Using 2 GPUs\n",
    "Test score -- loss: 0.0482255089384 |\n",
    "Test metrics -- accuracy: 0.9909 |\n",
    "Total Duration (322.038 sec)\n",
    "\n",
    "Using 4 GPUs\n",
    "Test score -- loss: 0.0681361730697 |\n",
    "Test metrics -- accuracy: 0.9908 |\n",
    "Total Duration (179.739 sec)\n",
    "\n",
    "Using 8 GPUs\n",
    "Test score -- loss: 0.0483050177622 |\n",
    "Test metrics -- accuracy: 0.9946 |\n",
    "Total Duration (111.724 sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUGS\n",
    "\n",
    "    swig/python detected a memory leak of type 'int64_t *', no destructor found.\n",
    "> https://github.com/tensorflow/tensorflow/issues/13359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
